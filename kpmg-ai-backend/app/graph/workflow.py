"""LangGraph workflow for KPMG AI Agent"""
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from .state import AgentState
from app.config import Config
import time


def orchestrator_node(state: AgentState) -> AgentState:
    """
    Node orchestrateur : d√©compose la mission en sections et initie le workflow.
    """
    # D√©finir les sections √† traiter
    sections = [
        "1.1 D√©finition & p√©rim√®tre",
        "1.2 Sizing (TAM / SAM / SOM)",
        "1.3 Segmentation",
        "1.4 Tendances & drivers",
        "1.5 Cha√Æne de valeur / R√©gulation",
        "2.1 Principaux acteurs",
        "2.2 Mod√®les √©conomiques",
        "2.3 Chiffres cl√©s des acteurs",
        "2.4 Facteurs cl√©s d'achat",
        "2.5 Positionnement relatif",
        "3.1 Synth√®se ex√©cutive",
        "3.2 Risques & zones d'incertitude",
        "3.3 Leviers de d√©veloppement",
        "3.4 Prochaines √©tapes"
    ]
    
    state["sections_to_process"] = sections
    state["completed_sections"] = []
    state["report_sections"] = []
    state["total_sections"] = len(sections)
    state["current_section_index"] = 0
    state["progress_percentage"] = 0.0
    state["start_time"] = time.time()
    state["current_step"] = "orchestrator"
    state["step_details"] = {"message": "Initialisation du workflow..."}
    
    return state


def process_section_node(state: AgentState) -> AgentState:
    """
    Node qui traite une section : d√©termine quelle section traiter et pr√©pare la requ√™te
    """
    sections = state.get("sections_to_process", [])
    completed = state.get("completed_sections", [])
    current_index = state.get("current_section_index", 0)
    
    # V√©rification de s√©curit√© : √©viter les boucles infinies
    if not sections or len(sections) == 0:
        state["current_step"] = "completed"
        state["progress_percentage"] = 1.0
        return state
    
    # V√©rifier si toutes les sections sont d√©j√† compl√©t√©es
    if len(completed) >= len(sections) or current_index >= len(sections):
        # Toutes les sections sont trait√©es
        state["current_step"] = "completed"
        state["progress_percentage"] = 1.0
        state["current_section_index"] = len(sections)  # S'assurer que l'index est √† jour
        return state
    
    # S'assurer que l'index ne d√©passe pas la taille de la liste
    if current_index >= len(sections):
        current_index = len(sections) - 1
    
    current_section = sections[current_index]
    state["current_section"] = current_section
    state["current_step"] = "process_section"
    
    # Construire la requ√™te pour cette section
    query = f"{current_section} pour le march√© {state['market_name']} en {state['geography']}"
    state["current_query"] = query
    
    # Mettre √† jour la progression
    progress = current_index / len(sections) if len(sections) > 0 else 1.0
    state["progress_percentage"] = progress
    state["step_details"] = {
        "message": f"Traitement de la section {current_section} ({current_index + 1}/{len(sections)})...",
        "section": current_section,
        "section_index": current_index + 1,
        "total_sections": len(sections)
    }
    
    return state


def cascade_research_node(state: AgentState) -> AgentState:
    """
    Node de recherche en cascade : INTERNE ‚Üí WEB ‚Üí ESTIMATION
    Avec logique sp√©ciale pour les sections n√©cessitant des donn√©es chiffr√©es
    """
    from app.tools.rag_tool import search_internal_knowledge
    from app.tools.linkup_search_tool import linkup_web_search
    from app.tools.estimation_tool import estimate_market_data
    import re
    
    query = state.get("current_query", "")
    section = state.get("current_section", "")
    market_name = state.get("market_name", "")
    geography = state.get("geography", "")
    
    source_history = []
    state["current_step"] = "cascade_research"
    
    # === SECTIONS QUI N√âCESSITENT OBLIGATOIREMENT DES DONN√âES CHIFFR√âES ===
    quantitative_sections = [
        "1.2 Sizing",  # TAM/SAM/SOM
        "1.3 Segmentation",
        "1.4 Tendances",
        "2.1 Principaux acteurs",
        "2.3 Chiffres cl√©s",
        "2.4 Facteurs",
        "2.5 Positionnement"
    ]
    requires_numbers = any(qs in section for qs in quantitative_sections)
    
    # === SECTIONS DE SYNTH√àSE qui utilisent les donn√©es des sections pr√©c√©dentes ===
    synthesis_sections = [
        "3.1 Synth√®se",
        "3.2 Risques",
        "3.3 Leviers",
        "3.4 Prochaines"
    ]
    is_synthesis = any(ss in section for ss in synthesis_sections)
    
    def has_numeric_data(content: str) -> bool:
        """V√©rifie si le contenu contient des donn√©es num√©riques exploitables"""
        if not content:
            return False
        numeric_patterns = [
            r'\d+[,.]?\d*\s*(milliards?|Md|billions?)\s*(d\')?euros?',
            r'\d+[,.]?\d*\s*(millions?|M)\s*(d\')?euros?',
            r'\d+[,.]?\d*\s*%',
            r'\d+[,.]?\d*\s*‚Ç¨',
            r'~?\d+[,.]?\d*\s*(Md‚Ç¨|M‚Ç¨)',
            r'(TAM|SAM|SOM)[^:]*:\s*~?\d+',
            r'part\s*(de\s*)?march[√©e][^:]*:\s*~?\d+',
        ]
        for pattern in numeric_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return True
        return False
    
    def has_useful_content(content: str) -> bool:
        """V√©rifie si le contenu est utile (pas juste des erreurs ou messages vides)"""
        if not content or len(content) < 100:
            return False
        # Patterns indiquant un contenu inutile
        useless_patterns = [
            r'aucun(e)?\s+(r√©sultat|donn√©e|information)',
            r'pas\s+de\s+(donn√©es?|r√©sultats?)',
            r'source.*n/a',
            r'erreur',
            r'malheureusement.*aucun'
        ]
        content_lower = content.lower()
        for pattern in useless_patterns:
            if re.search(pattern, content_lower):
                return False
        return True
    
    # === Pour les sections de SYNTH√àSE, utiliser les donn√©es des sections pr√©c√©dentes ===
    if is_synthesis:
        previous_sections = state.get("report_sections", [])
        if previous_sections:
            # Compiler un r√©sum√© des donn√©es des sections pr√©c√©dentes
            synthesis_context = f"DONN√âES DES SECTIONS PR√âC√âDENTES pour {market_name} en {geography}:\n\n"
            for prev_section in previous_sections:
                synthesis_context += f"### {prev_section.get('title', '')}:\n"
                synthesis_context += f"{prev_section.get('content', '')[:500]}...\n\n"
            
            state["synthesis_context"] = synthesis_context
            state["internal_result"] = {"content": synthesis_context, "score": 0.8}
            state["final_source"] = "SYNTHESE"
            state["confidence_score"] = 0.8
            state["source_history"] = [{"step": 0, "source": "SYNTHESE", "status": "compiled"}]
            state["has_numeric_data"] = True
            return state
    
    # === √âtape 1 : Recherche INTERNE ===
    state["step_details"] = {
        "message": f"Recherche INTERNE pour {section}...",
        "source": "INTERNE"
    }
    internal_result = search_internal_knowledge.invoke({"query": query})
    
    has_results = "Aucune information" not in internal_result and "configuration manquante" not in internal_result
    
    # Extraire le score de similarit√©
    best_similarity = 0.0
    if has_results:
        similarity_match = re.search(r'\[BEST_SIMILARITY:\s*([\d.]+)\]', internal_result)
        if similarity_match:
            best_similarity = float(similarity_match.group(1))
        else:
            similarity_matches = re.findall(r'Similarity:\s*([\d.]+)', internal_result)
            if similarity_matches:
                best_similarity = max(float(s) for s in similarity_matches)
    
    internal_has_numbers = has_numeric_data(internal_result) if has_results else False
    
    # Stocker le r√©sultat INTERNE pour le passer √† l'ESTIMATION si n√©cessaire
    state["internal_data_for_estimation"] = internal_result if has_results else ""
    
    source_history.append({
        "step": 1,
        "source": "INTERNE",
        "status": "found" if has_results and best_similarity > 0.80 else "not_found",
        "score": best_similarity,
        "has_numbers": internal_has_numbers
    })
    
    # Conditions pour accepter INTERNE
    content_length = len(internal_result) if has_results else 0
    is_content_sufficient = content_length > 150
    section_keywords = section.lower().split()
    content_lower = internal_result.lower() if has_results else ""
    keyword_match_count = sum(1 for kw in section_keywords if kw in content_lower and len(kw) > 3)
    is_section_relevant = keyword_match_count >= 2
    
    if requires_numbers:
        internal_valid = (has_results and best_similarity > 0.80 and 
                         is_content_sufficient and is_section_relevant and internal_has_numbers)
    else:
        internal_valid = (has_results and best_similarity > 0.80 and 
                         is_content_sufficient and is_section_relevant)
    
    if internal_valid:
        state["internal_result"] = {"content": internal_result, "score": best_similarity}
        state["final_source"] = "INTERNE"
        state["confidence_score"] = min(0.9, best_similarity * 0.95)
        state["source_history"] = source_history
        state["has_numeric_data"] = internal_has_numbers
        return state
    
    # === √âtape 2 : Recherche WEB ===
    state["step_details"] = {
        "message": f"Recherche WEB pour {section}...",
        "source": "WEB"
    }
    
    if requires_numbers:
        web_query = f"{query} chiffres donn√©es statistiques taille march√© parts de march√© {market_name} {geography}"
    else:
        web_query = query
    
    web_result = linkup_web_search.invoke({"query": web_query})
    web_has_numbers = has_numeric_data(web_result) if web_result else False
    web_is_useful = has_useful_content(web_result)
    
    # Stocker pour l'ESTIMATION
    state["web_data_for_estimation"] = web_result if web_is_useful else ""
    
    source_history.append({
        "step": 2,
        "source": "WEB",
        "status": "found" if web_is_useful else "not_found",
        "has_numbers": web_has_numbers
    })
    
    # Accepter WEB seulement si contenu utile ET (pas besoin de chiffres OU a des chiffres)
    if web_is_useful and (not requires_numbers or web_has_numbers):
        state["web_result"] = {"content": web_result, "score": 0.7}
        state["final_source"] = "WEB"
        state["confidence_score"] = 0.7
        state["source_history"] = source_history
        state["has_numeric_data"] = web_has_numbers
        return state
    
    # === √âtape 3 : ESTIMATION (toujours si on arrive ici) ===
    state["step_details"] = {
        "message": f"Estimation pour {section}...",
        "source": "ESTIMATION"
    }
    
    # Context enrichi avec les donn√©es INTERNE et WEB trouv√©es (m√™me partielles)
    context_parts = [f"March√©: {market_name}", f"G√©ographie: {geography}", f"Section: {section}"]
    
    # Ajouter les donn√©es INTERNE si disponibles
    internal_data = state.get("internal_data_for_estimation", "")
    if internal_data and len(internal_data) > 50:
        context_parts.append(f"\nDONN√âES INTERNES DISPONIBLES (√† utiliser comme base):\n{internal_data[:1000]}")
    
    # Ajouter les donn√©es WEB si disponibles
    web_data = state.get("web_data_for_estimation", "")
    if web_data and len(web_data) > 50:
        context_parts.append(f"\nDONN√âES WEB DISPONIBLES (√† utiliser comme r√©f√©rence):\n{web_data[:1000]}")
    
    context = "\n".join(context_parts)
    
    # Variables sp√©cifiques selon le type de section
    if "Sizing" in section or "TAM" in section:
        variables = f"TAM SAM SOM taille march√© {market_name} {geography} en milliards d'euros. IMPORTANT: Si des donn√©es internes mentionnent un TAM de 7 Md‚Ç¨, utilise cette valeur comme base."
    elif "Segmentation" in section:
        variables = f"Segmentation march√© {market_name} {geography} par cat√©gorie de produit avec pourcentages"
    elif "acteurs" in section.lower() or "Principaux" in section:
        variables = f"Parts de march√© principaux acteurs {market_name} {geography} avec noms r√©els et pourcentages"
    elif "Chiffres cl√©s" in section:
        variables = f"Chiffres cl√©s acteurs march√© {market_name} {geography} CA parts de march√©"
    elif "Tendances" in section:
        variables = f"Tendances et drivers march√© {market_name} {geography} avec donn√©es chiffr√©es sur la croissance"
    elif "Facteurs" in section:
        variables = f"Facteurs cl√©s d'achat march√© {market_name} {geography} avec importance relative en %"
    elif "Positionnement" in section:
        variables = f"Positionnement relatif acteurs march√© {market_name} {geography} mapping concurrentiel"
    else:
        variables = query
    
    estimation_result = estimate_market_data.invoke({
        "context": context,
        "variables": variables
    })
    
    source_history.append({
        "step": 3,
        "source": "ESTIMATION",
        "status": "estimated"
    })
    
    state["estimation_result"] = {"content": estimation_result, "score": 0.5}
    state["final_source"] = "ESTIMATION"
    state["confidence_score"] = 0.5
    state["source_history"] = source_history
    state["has_numeric_data"] = True
    
    return state


def report_generation_node(state: AgentState) -> AgentState:
    """
    Node de g√©n√©ration de rapport : assemble les sections avec formatage
    """
    from langchain_openai import ChatOpenAI
    
    state["current_step"] = "report_generation"
    section = state.get("current_section", "")
    
    state["step_details"] = {
        "message": f"G√©n√©ration du rapport pour {section}...",
        "section": section
    }
    
    llm = ChatOpenAI(
        model=Config.OPENAI_MODEL,
        temperature=0.3,
        api_key=Config.OPENAI_API_KEY
    )
    
    # R√©cup√©rer les r√©sultats de la cascade
    if state.get("final_source") == "INTERNE":
        content = state.get("internal_result", {}).get("content", "")
    elif state.get("final_source") == "WEB":
        content = state.get("web_result", {}).get("content", "")
    elif state.get("final_source") == "SYNTHESE":
        content = state.get("synthesis_context", "")
    else:
        content = state.get("estimation_result", {}).get("content", "")
    
    # Nettoyer le contenu RAG pour √©viter les r√©p√©titions
    # Si le contenu contient plusieurs chunks avec headers [Source: ...], on les consolide
    if state.get("final_source") == "INTERNE" and "[Source:" in content:
        # Extraire uniquement le contenu sans les headers r√©p√©titifs
        lines = content.split('\n')
        cleaned_lines = []
        skip_next = False
        for i, line in enumerate(lines):
            if line.strip().startswith('[Source:'):
                # Garder seulement le premier header de source
                if not any('[Source:' in prev_line for prev_line in cleaned_lines):
                    cleaned_lines.append(line)
                skip_next = False
            elif line.strip() == '---':
                # Ignorer les s√©parateurs entre chunks
                skip_next = True
            elif not skip_next:
                cleaned_lines.append(line)
        content = '\n'.join(cleaned_lines)
        # Ajouter un header de source unique au d√©but
        if content and not content.startswith('[Source:'):
            source_match = content.split('\n')[0] if content else ""
            if '[Source:' in source_match:
                content = source_match + '\n\n' + '\n'.join(cleaned_lines[1:]) if len(cleaned_lines) > 1 else content
    
    # Prompt diff√©renci√© selon la source
    final_source = state.get('final_source', 'UNKNOWN')
    confidence_score = state.get('confidence_score', 0.0)
    
    # Construire le prompt SANS f-strings pour √©viter les probl√®mes d'accolades
    # LangChain requiert {{ }} pour les accolades litt√©rales dans les templates
    
    graph_format_example = '```json\n{{"type": "pie", "title": "Titre", "data": {{"values": [55, 30, 15]}}, "labels": ["A", "B", "C"]}}\n```'
    
    if final_source == "INTERNE":
        system_prompt = """Tu es un consultant senior KPMG qui formate un rapport.

R√àGLE ABSOLUE : Tu NE DOIS PAS inventer de donn√©es. UNIQUEMENT les informations du contenu source.

Instructions strictes :
1. Extrais les donn√©es EXACTEMENT comme elles apparaissent
2. Cite les noms EXACTS (Nestl√© Purina, Royal Canin, pas Acteur A/B)
3. Si une donn√©e n'est PAS dans la source, √©cris "Donn√©e non disponible"
4. NE PAS compl√©ter avec tes connaissances g√©n√©rales

GRAPHIQUES : G√©n√®re un graphique UNIQUEMENT si tu as des DONN√âES CHIFFR√âES R√âELLES.
Format: """ + graph_format_example + """

Format de sortie :
- Titre de section
- Contenu structur√© avec les VRAIES donn√©es
- Tableaux avec les VRAIS chiffres si pr√©sents
- üü¢ Source Interne
- Score de confiance: """ + str(round(confidence_score, 2))
    
    elif final_source == "WEB":
        system_prompt = """Tu es un consultant senior KPMG qui formate un rapport.

Instructions :
1. Utilise les DONN√âES R√âELLES du contenu source web
2. Tu peux ajouter du contexte explicatif autour des donn√©es
3. Cite les VRAIS chiffres et noms d'entreprises trouv√©s
4. NE PAS inventer de donn√©es si elles ne sont pas dans la source

GRAPHIQUES : G√©n√®re un graphique UNIQUEMENT si tu as des DONN√âES CHIFFR√âES R√âELLES.
Format: """ + graph_format_example + """

Format de sortie :
- Titre de section
- Contenu structur√© avec donn√©es sourc√©es
- Tableaux avec donn√©es r√©elles si disponibles
- üîµ Source Web
- Score de confiance: """ + str(round(confidence_score, 2))
    
    elif final_source == "SYNTHESE":
        # PROMPT pour les sections de SYNTH√àSE (Partie 3)
        system_prompt = """Tu es un consultant senior KPMG qui r√©dige une section de synth√®se.

Cette section DOIT s'appuyer sur les donn√©es des sections pr√©c√©dentes du rapport.
Tu as acc√®s aux donn√©es compil√©es des parties 1 et 2.

Instructions :
1. Synth√©tise les informations cl√©s des sections pr√©c√©dentes
2. Mets en avant les chiffres importants (TAM, parts de march√©, etc.)
3. Identifie les tendances et conclusions principales
4. Pour les risques : identifie les zones d'incertitude bas√©es sur les scores de confiance faibles
5. Pour les leviers : propose des actions bas√©es sur les opportunit√©s identifi√©es
6. Pour les prochaines √©tapes : recommande des actions concr√®tes et si donn√©es insuffisantes, RECOMMANDE UN RDV EXPERT

NE PAS utiliser de formules math√©matiques ou LaTeX. Utilise du texte simple.

GRAPHIQUES : G√©n√®re des graphiques r√©capitulatifs si pertinent.
Format: """ + graph_format_example + """

Format de sortie :
- Titre de section
- Contenu structur√© bas√© sur les donn√©es des sections pr√©c√©dentes
- Points cl√©s en bullet points
- Recommandations concr√®tes
- üîµ Synth√®se
- Score de confiance: """ + str(round(confidence_score, 2))
    
    else:  # ESTIMATION
        system_prompt = """Tu es un consultant senior KPMG qui formate un rapport bas√© sur des estimations.

Le contenu provient d'un mod√®le d'ESTIMATION avec des hypoth√®ses m√©thodologiques.
IMPORTANT : Si des donn√©es internes sont fournies dans le contexte, utilise-les comme BASE pour tes estimations.

Instructions :
1. Formate les estimations de mani√®re professionnelle
2. Pr√©sente les chiffres estim√©s clairement avec des calculs simples en texte (PAS de LaTeX)
3. Indique les hypoth√®ses utilis√©es
4. Si une donn√©e interne existe (ex: TAM = 7 Md‚Ç¨), utilise-la plut√¥t que d'inventer

NE PAS utiliser de formules math√©matiques LaTeX (pas de \\text, \\times, etc.). Utilise du texte simple.
Exemple correct : "27 millions x 50% = 13.5 millions" (pas de [, ], \\text)

GRAPHIQUES : G√©n√®re des graphiques pour visualiser les estimations.
Format: """ + graph_format_example + """

Format de sortie :
- Titre de section
- Contenu structur√© avec les estimations chiffr√©es
- Calculs expliqu√©s en texte simple
- Tableaux avec les valeurs estim√©es
- Graphiques pour visualiser les donn√©es
- üü° Estimation
- Score de confiance: """ + str(round(confidence_score, 2)) + """
- Hypoth√®ses : liste des hypoth√®ses cl√©s"""
    
    # Le prompt user n'utilise PAS de variables LangChain, on passe tout en dur
    user_content = f"Section: {section}\nContenu source: {content}\n\nG√©n√®re la section format√©e. RAPPEL: utilise UNIQUEMENT les donn√©es r√©elles du contenu source."
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", user_content)
    ])
    
    try:
        chain = prompt | llm
        # Pas de variables √† passer car tout est d√©j√† int√©gr√© dans le prompt
        formatted_section = chain.invoke({})
        formatted_content = formatted_section.content if hasattr(formatted_section, 'content') else str(formatted_section)
    except Exception as e:
        # En cas d'erreur, essayer de formater au moins le d√©but du contenu
        error_msg = str(e)
        # Ne pas afficher le contenu source brut s'il est trop long ou contient des r√©p√©titions
        if len(content) > 500 or content.count('[Source:') > 1:
            # Extraire juste le d√©but du contenu pour √©viter les r√©p√©titions
            content_preview = content.split('\n\n---\n\n')[0] if '\n\n---\n\n' in content else content[:500]
            formatted_content = f"Erreur lors de la g√©n√©ration: {error_msg}\n\n[üü¢ INTERNE] Contenu source (extrait):\n{content_preview}..."
        else:
            formatted_content = f"Erreur lors de la g√©n√©ration: {error_msg}\n\n[üü¢ INTERNE] Contenu source:\n{content}"
    
    # Ajouter √† la liste des sections compl√©t√©es
    section_data = {
        "id": section,
        "title": section,
        "content": formatted_content,
        "source": state.get("final_source", "UNKNOWN"),
        "confidence_score": state.get("confidence_score", 0.0),
        "source_history": state.get("source_history", []),
        "can_deepen": True
    }
    
    completed = state.get("completed_sections", [])
    completed.append(section_data)
    state["completed_sections"] = completed
    
    report_sections = state.get("report_sections", [])
    report_sections.append(section_data)
    state["report_sections"] = report_sections
    
    # Mettre √† jour l'index et la progression
    current_index = state.get("current_section_index", 0)
    sections = state.get("sections_to_process", [])
    total = state.get("total_sections", len(sections) if sections else 1)
    
    # Incr√©menter l'index seulement si on n'a pas d√©pass√©
    if current_index < len(sections):
        state["current_section_index"] = current_index + 1
    else:
        state["current_section_index"] = len(sections)
    
    # Mettre √† jour la progression
    state["progress_percentage"] = min(1.0, (current_index + 1) / total) if total > 0 else 1.0
    state["total_sections"] = total
    
    return state


def expert_recommendation_node(state: AgentState) -> AgentState:
    """
    Node de d√©tection d'incertitude et recommandation d'expert SP√âCIFIQUE AU MARCH√â
    """
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    
    state["current_step"] = "expert_recommendation"
    state["step_details"] = {"message": "Analyse des zones d'incertitude..."}
    
    recommendations = []
    report_sections = state.get("report_sections", [])
    market_name = state.get("market_name", "")
    geography = state.get("geography", "")
    
    llm = ChatOpenAI(
        model=Config.OPENAI_MODEL,
        temperature=0.3,
        api_key=Config.OPENAI_API_KEY
    )
    
    for section in report_sections:
        confidence = section.get("confidence_score", 1.0)
        # Recommander un expert pour les sections avec score < 0.7 (pas seulement < 0.5)
        if confidence < 0.7:
            section_title = section.get('title', 'Unknown')
            source = section.get('source', 'UNKNOWN')
            
            system_msg = """Tu g√©n√®res des recommandations d'expert pour les zones d'incertitude d'une √©tude de march√©.

R√àGLE IMPORTANTE : L'expert recommand√© doit √™tre un SP√âCIALISTE DU MARCH√â √âTUDI√â, pas un expert g√©n√©raliste.

Pour chaque zone avec score < 0.5, g√©n√®re :
1. **Profil d'expert du march√©** :
   - Doit √™tre un expert du secteur sp√©cifique (ex: si march√© Pet Care ‚Üí expert industrie Pet Care/animalerie)
   - Exemples de profils : Directeur d'une entreprise du secteur, Analyste sectoriel sp√©cialis√©, Consultant sp√©cialiste du march√©, Responsable √©tudes de march√© dans une entreprise leader
   - PAS un expert g√©n√©raliste (pas "expert marketing", "expert comportement consommateur")

2. **Guide d'entretien structur√©** :
   - 5-7 questions sp√©cifiques au march√© √©tudi√©
   - Questions sur les donn√©es manquantes de la section
   - Focus sur les insights terrain et donn√©es propri√©taires"""
            
            user_msg = f"Section: {section_title}\nMarch√© √©tudi√©: {market_name}\nG√©ographie: {geography}\nScore de confiance: {confidence}\nSource utilis√©e: {source}\n\nG√©n√®re la recommandation d'expert SP√âCIFIQUE au march√© {market_name}."
            
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_msg),
                ("user", user_msg)
            ])
            
            try:
                chain = prompt | llm
                recommendation = chain.invoke({})
                rec_content = recommendation.content if hasattr(recommendation, 'content') else str(recommendation)
                
                recommendations.append({
                    "section_id": section.get("id", ""),
                    "section_title": section.get("title", ""),
                    "recommendation": rec_content
                })
            except Exception as e:
                recommendations.append({
                    "section_id": section.get("id", ""),
                    "section_title": section.get("title", ""),
                    "recommendation": f"Erreur lors de la g√©n√©ration de recommandation: {str(e)}"
                })
    
    state["expert_recommendations"] = recommendations
    return state


def should_continue(state: AgentState) -> str:
    """
    D√©termine si on doit continuer √† traiter des sections ou terminer
    """
    sections = state.get("sections_to_process", [])
    completed = state.get("completed_sections", [])
    current_index = state.get("current_section_index", 0)
    
    # V√©rifier si toutes les sections sont compl√©t√©es
    # On utilise √† la fois l'index et le nombre de sections compl√©t√©es pour plus de robustesse
    if len(completed) >= len(sections) or current_index >= len(sections):
        return "expert_recommendation"
    else:
        return "process_section"


def create_workflow_graph():
    """
    Cr√©e le graph LangGraph avec tous les nodes
    """
    workflow = StateGraph(AgentState)
    
    # Ajouter les nodes
    workflow.add_node("orchestrator", orchestrator_node)
    workflow.add_node("process_section", process_section_node)
    workflow.add_node("cascade_research", cascade_research_node)
    workflow.add_node("report_generation", report_generation_node)
    workflow.add_node("expert_recommendation", expert_recommendation_node)
    
    # D√©finir le flux
    workflow.set_entry_point("orchestrator")
    
    workflow.add_edge("orchestrator", "process_section")
    workflow.add_conditional_edges(
        "process_section",
        should_continue,
        {
            "process_section": "cascade_research",
            "expert_recommendation": "expert_recommendation"
        }
    )
    workflow.add_edge("cascade_research", "report_generation")
    workflow.add_edge("report_generation", "process_section")  # Loop pour traiter la section suivante
    workflow.add_edge("expert_recommendation", END)
    
    # Compiler le graph avec une limite de r√©cursion plus √©lev√©e
    # (par d√©faut 25, on met 50 pour g√©rer jusqu'√† 50 sections)
    app = workflow.compile()
    
    # Configurer la limite de r√©cursion (si support√© par la version de LangGraph)
    try:
        # Certaines versions de LangGraph supportent checkpointer avec limite
        app = app.with_config({"recursion_limit": 50})
    except Exception:
        # Si la m√©thode n'existe pas, on continue sans
        pass
    
    return app
